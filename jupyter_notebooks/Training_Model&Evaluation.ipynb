{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddd163d2-4a35-441f-8882-8c5d7a8b38cd",
   "metadata": {},
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e603eb1-f392-4ead-841e-f6f34211915d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"amazon_data_train.json\", \"r\") as read_file:\n",
    "    train_data = json.load(read_file)\n",
    " \n",
    "with open(\"amazon_data_test.json\", \"r\") as read_file:\n",
    "    test_data = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "34e47644-b14e-44a1-8ef4-4ac225ffc622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, BertForQuestionAnswering, AdamW\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cca8152a-624c-4086-82ca-2a183d3c0c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': 'Nokia C12 Android 12 (Go Edition) Smartphone, All-Day Battery, 4GB RAM (2GB RAM + 2GB Virtual RAM) + 64GB Capacity | Light Mint',\n",
       " 'qas': [{'id': '00001',\n",
       "   'is_impossible': False,\n",
       "   'question': 'What is the operating system of the Nokia C12 smartphone?',\n",
       "   'answers': [{'text': 'Android 12 (Go Edition)', 'answer_start': 10}]},\n",
       "  {'id': '00002',\n",
       "   'is_impossible': False,\n",
       "   'question': 'How much RAM does the Nokia C12 have?',\n",
       "   'answers': [{'text': '4GB', 'answer_start': 63}]},\n",
       "  {'id': '00003',\n",
       "   'is_impossible': False,\n",
       "   'question': 'Does the Nokia C12 have virtual RAM?',\n",
       "   'answers': [{'text': '(2GB RAM + 2GB Virtual RAM)', 'answer_start': 71}]},\n",
       "  {'id': '00004',\n",
       "   'is_impossible': False,\n",
       "   'question': 'What is the total capacity of the Nokia C12?',\n",
       "   'answers': [{'text': '64GB', 'answer_start': 101}]},\n",
       "  {'id': '00005',\n",
       "   'is_impossible': False,\n",
       "   'question': 'What is the color option available for the Nokia C12?',\n",
       "   'answers': [{'text': 'Light Mint', 'answer_start': 117}]}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ac7d83-f80f-45d5-a716-a3a75f0759a0",
   "metadata": {},
   "source": [
    "# Creating Training and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b7d4733-6411-4a32-817e-174a9f079cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=384):\n",
    "        self.examples = []\n",
    "        \n",
    "        for item in data:\n",
    "            context = item['context']\n",
    "            for qa in item['qas']:\n",
    "                # Skip impossible questions during training\n",
    "                if qa['is_impossible']:\n",
    "                    continue\n",
    "                    \n",
    "                question = qa['question']\n",
    "                answer_text = qa['answers'][0]['text']\n",
    "                answer_start = qa['answers'][0]['answer_start']\n",
    "                \n",
    "                # Save the example\n",
    "                self.examples.append({\n",
    "                    'context': context,\n",
    "                    'question': question,\n",
    "                    'answer_text': answer_text,\n",
    "                    'answer_start': answer_start,\n",
    "                    'id': qa['id']\n",
    "                })\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.examples[idx]\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            example['question'],\n",
    "            example['context'],\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation='only_second',\n",
    "            return_offsets_mapping=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Get offsets before removing from encoding\n",
    "        offsets = encoding.offset_mapping[0].numpy()\n",
    "        \n",
    "        # Find start and end positions\n",
    "        start_pos = example['answer_start']\n",
    "        end_pos = start_pos + len(example['answer_text'])\n",
    "        \n",
    "        # Map character positions to token positions\n",
    "        start_token = end_token = 0\n",
    "        \n",
    "        for i, (offset_start, offset_end) in enumerate(offsets):\n",
    "            if offset_start <= start_pos < offset_end:\n",
    "                start_token = i\n",
    "            if offset_start < end_pos <= offset_end:\n",
    "                end_token = i\n",
    "                break\n",
    "        \n",
    "        # Store example ID for evaluation\n",
    "        example_id = example['id']\n",
    "        \n",
    "        # Remove offset mapping as it's not needed for training\n",
    "        encoding.pop('offset_mapping')\n",
    "        \n",
    "        # Add start and end positions\n",
    "        encoding['start_positions'] = torch.tensor([start_token])\n",
    "        encoding['end_positions'] = torch.tensor([end_token])\n",
    "        \n",
    "        # Convert to individual tensors rather than batched\n",
    "        result = {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "        result['example_id'] = example_id\n",
    "        \n",
    "        return result\n",
    "\n",
    "class QAInferenceDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=384):\n",
    "        self.examples = []\n",
    "        self.contexts = {}\n",
    "        \n",
    "        for item in data:\n",
    "            context = item['context']\n",
    "            for qa in item['qas']:\n",
    "                question = qa['question']\n",
    "                \n",
    "                # Save the example\n",
    "                self.examples.append({\n",
    "                    'context': context,\n",
    "                    'question': question,\n",
    "                    'id': qa['id']\n",
    "                })\n",
    "                # Save context for reference\n",
    "                self.contexts[qa['id']] = context\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.examples[idx]\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            example['question'],\n",
    "            example['context'],\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation='only_second',\n",
    "            return_offsets_mapping=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Store context, offset mapping and ID for generating answers\n",
    "        offset_mapping = encoding.offset_mapping.squeeze(0).numpy()\n",
    "        example_id = example['id']\n",
    "        \n",
    "        # Remove offset mapping from encoding\n",
    "        encoding.pop('offset_mapping')\n",
    "        \n",
    "        # Convert to individual tensors rather than batched\n",
    "        result = {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "        result['example_id'] = example_id\n",
    "        result['offset_mapping'] = offset_mapping\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a777d40-9486-4b99-b341-2e7bcccaffc3",
   "metadata": {},
   "source": [
    "# Importing the Bert model\n",
    "\n",
    "bert-base-uncased model has approximately 110 million parameters. Here's the exact breakdown:\n",
    "\n",
    "BERT-base Architecture Specs:\n",
    "Layers: 12\n",
    "\n",
    "Hidden Size: 768\n",
    "\n",
    "Attention Heads: 12\n",
    "\n",
    "Total Parameters: ~110M\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9c3c1ad-35ac-4a94-b4e2-3f797ea39808",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForQuestionAnswering(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize tokenizer and model\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "model = BertForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# Set up device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731e4d7f-d92f-4252-995a-d7086b656b11",
   "metadata": {},
   "source": [
    "# Creatig the Datasets using the Dataloader Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dab10f8b-14b4-48a7-a84f-4b800c81d2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 73 examples\n",
      "Validating with 9 examples\n",
      "Testing with 31 examples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = QADataset(train_data, tokenizer)\n",
    "\n",
    "# Split training data for validation\n",
    "train_size = int(0.9 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_subset, val_subset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# Create test dataset for inference and evaluation\n",
    "test_dataset = QAInferenceDataset(test_data, tokenizer)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "print(f\"Training with {len(train_subset)} examples\")\n",
    "print(f\"Validating with {len(val_subset)} examples\")\n",
    "print(f\"Testing with {len(test_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86269b5-c847-4b1d-a80c-2c0ed5a254f5",
   "metadata": {},
   "source": [
    "# Model Definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c34966ba-4ec7-4eda-9089-9f812789ff3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, train_loader, eval_loader, device, epochs=3, lr=3e-5):\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    best_eval_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Training]\")\n",
    "        for batch in progress_bar:\n",
    "            # Remove example_id as it's not needed for training\n",
    "            example_ids = batch.pop('example_id')\n",
    "            \n",
    "            # Move to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        eval_loss = 0\n",
    "        \n",
    "        progress_bar = tqdm(eval_loader, desc=f\"Epoch {epoch+1}/{epochs} [Evaluation]\")\n",
    "        for batch in progress_bar:\n",
    "            example_ids = batch.pop('example_id')\n",
    "            \n",
    "            # Move to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "            \n",
    "            eval_loss += loss.item()\n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        avg_eval_loss = eval_loss / len(eval_loader)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Eval Loss: {avg_eval_loss:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_eval_loss < best_eval_loss:\n",
    "            best_eval_loss = avg_eval_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            print(f\"New best model saved with eval loss: {avg_eval_loss:.4f}\")\n",
    "    \n",
    "    # Load best model for return\n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a268a7a1-545f-4770-8aab-8ce834ec09bc",
   "metadata": {},
   "source": [
    "# Training and Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bb6a6be-73b6-46e1-b9e3-b7bef62d6ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lovis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 [Training]: 100%|█████████████████████████████████████████████████| 10/10 [04:09<00:00, 24.98s/it, loss=3.77]\n",
      "Epoch 1/5 [Evaluation]: 100%|█████████████████████████████████████████████████| 2/2 [00:12<00:00,  6.11s/it, loss=3.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Train Loss: 4.6003, Eval Loss: 3.3538\n",
      "New best model saved with eval loss: 3.3538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 [Training]: 100%|█████████████████████████████████████████████████| 10/10 [04:05<00:00, 24.52s/it, loss=2.83]\n",
      "Epoch 2/5 [Evaluation]: 100%|█████████████████████████████████████████████████| 2/2 [00:14<00:00,  7.08s/it, loss=2.65]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Train Loss: 3.3533, Eval Loss: 2.8259\n",
      "New best model saved with eval loss: 2.8259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 [Training]: 100%|█████████████████████████████████████████████████| 10/10 [04:10<00:00, 25.03s/it, loss=2.09]\n",
      "Epoch 3/5 [Evaluation]: 100%|██████████████████████████████████████████████████| 2/2 [00:13<00:00,  6.57s/it, loss=2.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Train Loss: 2.7757, Eval Loss: 2.4090\n",
      "New best model saved with eval loss: 2.4090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 [Training]: 100%|█████████████████████████████████████████████████| 10/10 [03:58<00:00, 23.85s/it, loss=1.79]\n",
      "Epoch 4/5 [Evaluation]: 100%|█████████████████████████████████████████████████| 2/2 [00:13<00:00,  6.55s/it, loss=1.76]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Train Loss: 2.3293, Eval Loss: 1.9509\n",
      "New best model saved with eval loss: 1.9509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 [Training]: 100%|█████████████████████████████████████████████████| 10/10 [03:59<00:00, 23.99s/it, loss=1.89]\n",
      "Epoch 5/5 [Evaluation]: 100%|█████████████████████████████████████████████████| 2/2 [00:13<00:00,  6.82s/it, loss=2.22]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Train Loss: 1.8098, Eval Loss: 1.9901\n",
      "Model saved to C:\\Users\\lovis\\Desktop\\Recommendation_system\\Bert_FineTuned\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "model = train_model(model, train_loader, val_loader, device, epochs=5)\n",
    "\n",
    "# Save the model\n",
    "model_save_path = r'C:\\Users\\lovis\\Desktop\\Recommendation_system\\Bert_FineTuned'\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a91c1a-3fc3-4648-8e56-7515fa4338d1",
   "metadata": {},
   "source": [
    "# Evalutation Function for testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f2a3bb4-1546-434f-91f6-15b420c17b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate_model(model, test_loader, tokenizer, device, test_data):\n",
    "    model.eval()\n",
    "    all_predictions = {}\n",
    "    ground_truth = {}\n",
    "    \n",
    "    # Extract ground truth answers from test data\n",
    "    for item in test_data:\n",
    "        for qa in item['qas']:\n",
    "            if not qa['is_impossible']:\n",
    "                ground_truth[qa['id']] = qa['answers'][0]['text']\n",
    "    \n",
    "    progress_bar = tqdm(test_loader, desc=\"Evaluating\")\n",
    "    for batch in progress_bar:\n",
    "        example_ids = batch.pop('example_id')\n",
    "        offset_mappings = batch.pop('offset_mapping')\n",
    "        \n",
    "        # Move to device\n",
    "        inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            start_logits = outputs.start_logits\n",
    "            end_logits = outputs.end_logits\n",
    "        \n",
    "        # Get predictions\n",
    "        for i, example_id in enumerate(example_ids):\n",
    "            # Get the most likely start and end indices\n",
    "            start_idx = torch.argmax(start_logits[i]).item()\n",
    "            end_idx = torch.argmax(end_logits[i]).item()\n",
    "            \n",
    "            # Make sure end_idx >= start_idx\n",
    "            if end_idx < start_idx:\n",
    "                end_idx = start_idx\n",
    "            \n",
    "            # Get the mapped tokens\n",
    "            offsets = offset_mappings[i]\n",
    "            \n",
    "            # Only consider tokens that are not special tokens\n",
    "            # CLS token is at index 0, so start from 1\n",
    "            if start_idx <= 0:\n",
    "                start_idx = 1\n",
    "            \n",
    "            # Find the start and end character positions in the original context\n",
    "            if start_idx < len(offsets) and end_idx < len(offsets):\n",
    "                start_char = offsets[start_idx][0].item()\n",
    "                end_char = offsets[end_idx][1].item()\n",
    "                \n",
    "                # Get the context and extract the predicted answer\n",
    "                context = test_loader.dataset.contexts[example_id]\n",
    "                if start_char < len(context) and end_char <= len(context):\n",
    "                    predicted_answer = context[start_char:end_char]\n",
    "                    all_predictions[example_id] = predicted_answer\n",
    "    \n",
    "    # Calculate metrics\n",
    "    exact_match = 0\n",
    "    f1_scores = []\n",
    "    \n",
    "    for qid, true_answer in ground_truth.items():\n",
    "        if qid in all_predictions:\n",
    "            prediction = all_predictions[qid]\n",
    "            \n",
    "            # Exact match\n",
    "            if prediction.lower() == true_answer.lower():\n",
    "                exact_match += 1\n",
    "            \n",
    "            # F1 score (token overlap)\n",
    "            true_tokens = set(true_answer.lower().split())\n",
    "            pred_tokens = set(prediction.lower().split())\n",
    "            \n",
    "            if not true_tokens and not pred_tokens:\n",
    "                f1_scores.append(1.0)\n",
    "            elif not true_tokens or not pred_tokens:\n",
    "                f1_scores.append(0.0)\n",
    "            else:\n",
    "                common_tokens = true_tokens.intersection(pred_tokens)\n",
    "                precision = len(common_tokens) / len(pred_tokens) if pred_tokens else 0\n",
    "                recall = len(common_tokens) / len(true_tokens) if true_tokens else 0\n",
    "                \n",
    "                f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n",
    "                f1_scores.append(f1)\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    em_score = exact_match / len(ground_truth) if ground_truth else 0\n",
    "    avg_f1 = sum(f1_scores) / len(f1_scores) if f1_scores else 0\n",
    "    \n",
    "    return {\n",
    "        'exact_match': em_score,\n",
    "        'f1': avg_f1,\n",
    "        'predictions': all_predictions\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be69ca37-343a-426a-9fc1-187062f773e6",
   "metadata": {},
   "source": [
    "# Evaluation on Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1f41ce1-425d-4b89-bf89-9fbdbb3beb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████| 4/4 [00:42<00:00, 10.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match: 0.2222\n",
      "F1 Score: 0.4937\n",
      "Predictions saved to qa_predictions.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating on test set...\")\n",
    "eval_results = evaluate_model(model, test_loader, tokenizer, device, test_data)\n",
    "print(f\"Exact Match: {eval_results['exact_match']:.4f}\")\n",
    "print(f\"F1 Score: {eval_results['f1']:.4f}\")\n",
    "\n",
    "# Save predictions to file\n",
    "with open('qa_predictions.json', 'w') as f:\n",
    "    json.dump(eval_results['predictions'], f, indent=2)\n",
    "print(\"Predictions saved to qa_predictions.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd9e463-20fe-42f9-a268-2ac9a1a3a5fd",
   "metadata": {},
   "source": [
    "# Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68c610c8-a9cf-4e4f-a216-bd7ba225db2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, tokenizer, context, questions, device):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    for question in questions:\n",
    "        # Tokenize\n",
    "        inputs = tokenizer(\n",
    "            question,\n",
    "            context,\n",
    "            max_length=384,\n",
    "            padding='max_length',\n",
    "            truncation='only_second',\n",
    "            return_tensors='pt',\n",
    "            return_offsets_mapping=True\n",
    "        )\n",
    "        \n",
    "        offset_mapping = inputs.pop('offset_mapping')\n",
    "        \n",
    "        # Move to device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            start_logits = outputs.start_logits\n",
    "            end_logits = outputs.end_logits\n",
    "        \n",
    "        # Get the most likely start and end indices\n",
    "        start_idx = torch.argmax(start_logits[0]).item()\n",
    "        end_idx = torch.argmax(end_logits[0]).item()\n",
    "        \n",
    "        # Make sure end_idx >= start_idx\n",
    "        if end_idx < start_idx:\n",
    "            end_idx = start_idx\n",
    "        \n",
    "        # Get the mapped tokens\n",
    "        offsets = offset_mapping[0].cpu().numpy()\n",
    "        \n",
    "        # Only consider tokens that are not special tokens\n",
    "        if start_idx <= 0:\n",
    "            start_idx = 1\n",
    "        \n",
    "        # Find the start and end character positions in the original context\n",
    "        if start_idx < len(offsets) and end_idx < len(offsets):\n",
    "            start_char = offsets[start_idx][0].item()\n",
    "            end_char = offsets[end_idx][1].item()\n",
    "            \n",
    "            # Extract the predicted answer\n",
    "            if start_char < len(context) and end_char <= len(context):\n",
    "                predicted_answer = context[start_char:end_char]\n",
    "                results.append({\n",
    "                    'question': question,\n",
    "                    'predicted_answer': predicted_answer,\n",
    "                    'confidence': float(torch.max(start_logits[0]).item() + torch.max(end_logits[0]).item()) / 2\n",
    "                })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923f588b-6d38-49b9-a547-b4026d4c9770",
   "metadata": {},
   "source": [
    "# Sample Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "497db698-ff31-46a4-bac2-1cc62b1d017f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running sample inference...\n",
      "Sample context: Redmi Note 11 (Space Black, 4GB RAM, 64GB Storage)|90Hz FHD+ AMOLED Display | QualcommÂ® Snapdragonâ„¢ 680-6nm | 33W Charger Included\n",
      "Q: What is the model name of the Redmi smartphone?\n",
      "A: Redmi Note 11 (Sp (Confidence: 4.6656)\n",
      "--------------------------------------------------\n",
      "Q: What is the color option available for the Redmi Note 11?\n",
      "A: ce Black, 4GB RA (Confidence: 5.5445)\n",
      "--------------------------------------------------\n",
      "Q: How much RAM does the Redmi Note 11 have?\n",
      "A: Redmi Note 11 (Space Black, (Confidence: 3.3747)\n",
      "--------------------------------------------------\n",
      "Q: What is the storage capacity of the Redmi Note 11?\n",
      "A: 64GB (Confidence: 3.4532)\n",
      "--------------------------------------------------\n",
      "Q: What is the display feature of the Redmi Note 11?\n",
      "A: 90Hz FHD+ AMOLED Display (Confidence: 4.8998)\n",
      "--------------------------------------------------\n",
      "Q: What is included in the package of the Redmi Note 11?\n",
      "A: Redmi Note 11 (S (Confidence: 2.7101)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Run inference on a sample\n",
    "print(\"\\nRunning sample inference...\")\n",
    "sample_context = test_data[0]['context']\n",
    "sample_questions = [qa['question'] for qa in test_data[0]['qas']]\n",
    "\n",
    "inference_results = run_inference(model, tokenizer, sample_context, sample_questions, device)\n",
    "\n",
    "print(f\"Sample context: {sample_context}\")\n",
    "for result in inference_results:\n",
    "    print(f\"Q: {result['question']}\")\n",
    "    print(f\"A: {result['predicted_answer']} (Confidence: {result['confidence']:.4f})\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd7efbc-fd04-4800-80f8-0bc49dbe035d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b22b700-6626-4ba2-9495-9b5b7d455383",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9af8e9-b311-492b-a512-4b9bf448c454",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15ccadc-2849-4032-b87a-c1f5f3bb58ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7194f14c-bf33-4f71-be63-bf5fd1d2c802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a800fc-d05b-4ca8-a66d-2a2672fc9082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bd7e9c-8826-4e43-a6fc-e517641e5993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363b01fd-6dd6-461c-b885-aa7dbbbdf9ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e14f35f-8019-4fa4-85d8-b7ef2696cd34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2796fa6d-cb0b-4f72-a124-c066dd9cbe55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0327fca-4266-44d4-9042-af6f09d2e537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386bce53-8e96-4cab-ac61-57143aceb514",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f244846b-fefe-4dd3-a986-69a2426969b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea9c4f7-b95c-48db-9b58-5324e9950272",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870b84c6-30bf-40c2-9e85-feb38eecf5ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060fd411-923f-4d3a-b10a-6244e952fd61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270be491-919e-4d16-ae5a-b4e01df79cdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
